{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd2510-9a66-4410-9347-fa1349a1140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "pd.set_option(\"display.max_rows\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7c62f-44a8-4108-ac0d-c80e8336b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade jupyterlab ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474b325-e526-4dab-9ffd-3b67fac66fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter lab build\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480bf7b0-87f1-4a62-abe2-3509bfca97b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', encoding='latin1')\n",
    "# we will copy the dataframe for some actions with 'Investors column you can see it below'\n",
    "df2=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962ee61-99e6-4687-8821-e115b5e6477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "#we are dealing with high dimensional data, based on this I create a custom info method and customize the describe method to \"summary statistics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee66b34-12d6-4d83-a010-087d69051502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_info(df):\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    for i, col in enumerate(df.columns):\n",
    "        non_null_count = len(df)-df[col].isna().sum()\n",
    "        dtype = df[col].dtype\n",
    "        print(f\" {i}    {col:<50} {non_null_count} non-null    {dtype}\")\n",
    "    \n",
    "def summary_statistics(df):\n",
    "    # Summary statistics for numeric columns\n",
    "    print(\"Summary statistics for numeric columns:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Unique value counts for object (categorical) columns\n",
    "    print(\"\\nUnique value counts for object (categorical) columns:\")\n",
    "    for column in df.select_dtypes(include=['object']).columns:\n",
    "        print(f\"\\nColumn: {column}\")\n",
    "        print(f\"{df[column].nunique()} unique values\")\n",
    "        print(df[column].value_counts())\n",
    "\n",
    "def find_weak_columns(df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Find columns in a DataFrame with more than a specified threshold of missing values.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame to analyze.\n",
    "    threshold (float): The threshold for the proportion of missing values (default is 0.5).\n",
    "\n",
    "    Returns:\n",
    "    list: A list of column names with more than the specified threshold of missing values.\n",
    "    \"\"\"\n",
    "    columns_to_drop = []\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    for column in df.columns:\n",
    "        missing_values = df[column].isna().sum()\n",
    "        missing_ratio = missing_values / total_rows\n",
    "        if missing_ratio > threshold:\n",
    "            columns_to_drop.append(column)\n",
    "    \n",
    "    return columns_to_drop\n",
    "\n",
    "def count_nan_values(df, features):\n",
    "    nan_counts = {}\n",
    "    for feature in features:\n",
    "        nan_count = df[feature].isna().sum()\n",
    "        nan_counts[feature] = nan_count\n",
    "    \n",
    "    # Sort the nan_counts dictionary by values in descending order\n",
    "    nan_counts = dict(sorted(nan_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return nan_counts\n",
    "\n",
    "def droping(df):\n",
    "    df.drop(columns=(['Est. Founding Date']+['Company_Name','Short Description of company profile',\n",
    "                                            'Specialization of highest education','Investors','Industry of company']\n",
    "                                          +['Employees per year of company existence','Last round of funding received (in milionUSD)',\n",
    "                                          'Time to 1st investment (in months)',\n",
    "                                            'Experience in Fortune 100 organizations','Experience in Fortune 1000 organizations']+\n",
    "                                           ['Last Funding Date']),inplace=True)\n",
    "    df.drop(columns=find_weak_columns(df),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20974e5e-d03d-4191-b8e7-49c21f5327b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19dfd12-c935-4560-bff8-6a52afcd6c83",
   "metadata": {},
   "source": [
    "#we see that we have many columns with a high content of missing values, so in the droping function we will remove all columns with more then 50% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3ed4fe-2977-4775-b3d8-7e3efad6ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ac9ad-2023-4cbd-b043-ae8cbd96e724",
   "metadata": {},
   "source": [
    "1. **we see that in columns except NAN there is NO_INFO instead of missing values, we will replace it with NAN**\n",
    "2. **we see that there are visually many more numeric columns here than CUSTOM_INFO showed, based on this we will group all these columns into a variable and change the data types of these columns to numeric ones**\n",
    "3. **we will also remove 'Est. Founding Date' because leaving this and 'year of founding' we will have leakage. we remove concretely this one because it has more missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c1f489-a416-40f9-9ab8-be03d9bc582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(\"No Info\", np.nan, inplace=True)\n",
    "numeric_visual=['Age of company in years','Internet Activity Score','Employee Count','year of founding','Employees count MoM change','Last Funding Amount',\n",
    "         'Number of Investors in Seed','Number of Investors in Angel and or VC','Number of Co-founders','Number of of advisors',\n",
    "         'Team size Senior leadership','Team size all employees','Number of of repeat investors','Years of education',\n",
    "         'Renowned in professional circle','Number of Recognitions for Founders and Co-founders','Skills score',\n",
    "         'google page rank of company website','Industry trend in investing','Number of Direct competitors',\n",
    "         'Employees per year of company existence','Last round of funding received (in milionUSD)','Time to 1st investment (in months)',\n",
    "         'Avg time to investment - average across all rounds, measured from previous investment','Percent_skill_Entrepreneurship',\n",
    "         'Percent_skill_Operations','Percent_skill_Engineering','Percent_skill_Marketing','Percent_skill_Leadership',\n",
    "         'Percent_skill_Data Science','Percent_skill_Business Strategy','Percent_skill_Product Management','Percent_skill_Sales',\n",
    "         'Percent_skill_Domain','Percent_skill_Law','Percent_skill_Consulting','Percent_skill_Finance','Percent_skill_Investment',\n",
    "         'Renown score','Experience in Fortune 500 organizations','Experience in Fortune 100 organizations',\n",
    "         'Experience in Fortune 1000 organizations']\n",
    "numeric_columns = []\n",
    "    # Iterate through columns in the DataFrame\n",
    "for col in df.columns:\n",
    "    # Check if the data type of the column is numeric\n",
    "    if pd.api.types.is_numeric_dtype(df[col]):\n",
    "        # If the data type is numeric, add the column name to the list\n",
    "        numeric_columns.append(col)\n",
    "        # aystex arden stanum enq arden isk numericner@ \n",
    "extracted_columns = [column for column in numeric_visual if column not in numeric_columns]\n",
    "for col in extracted_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a5d09-b6ac-4d34-968f-e4894c823025",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1dea5f-4dc5-403f-8404-91f228ff3dae",
   "metadata": {},
   "source": [
    "looking at the unique values ​​of columns with string types in summary_statistics(), it is noticeable that there are the same values ​​with capital and small letters, so we will capitalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435dcb3-fa70-4d5d-821d-1ddd7ac8679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    # Capitalize data points in object columns\n",
    "    df[col] = df[col].str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6708039-7afa-4527-9b07-0af520d50578",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bec89cc-1b6e-4112-b878-4fd4788d668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e90c8-b658-4bc9-832d-9398e490ee5d",
   "metadata": {},
   "source": [
    "# Now we will investigate High Cardinality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f281d-8e51-4ae4-80e3-12d6915972ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_un_val_cat_var(df):\n",
    "    \"\"\"\n",
    "    Visualizes the number of unique values for each categorical variable\n",
    "    in descending order using a histogram.\n",
    "    \"\"\"\n",
    "    # Filter categorical variables\n",
    "    categorical_vars = df.select_dtypes(include=['object'])\n",
    "    \n",
    "    # Count unique values for each categorical variable\n",
    "    unique_value_counts = categorical_vars.nunique().sort_values(ascending=False)\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    unique_value_counts.plot(kind='bar')\n",
    "    plt.title('Number of Unique Values for Categorical Variables')\n",
    "    plt.xlabel('Categorical Variables')\n",
    "    plt.ylabel('Number of Unique Values')\n",
    "    # plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()\n",
    "vis_un_val_cat_var(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf48eb-54b6-4bf5-b5b3-7cfc46422b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vars = df.select_dtypes(include=['object'])\n",
    "first_7_unique_value_counts = categorical_vars.nunique().sort_values(ascending=False)[:8]\n",
    "print(first_7_unique_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45c987-19f1-46b3-a93c-fd0b3f75f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nan_values(df,first_7_unique_value_counts.index.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1526e9ac-504d-4e6f-8d34-91913a378c0f",
   "metadata": {},
   "source": [
    "# Decisions:\n",
    "\n",
    "1. **Drop 'Company_Name'** due to very high cardinality.\n",
    "\n",
    "2. From the columns **'Short Description of company profile'**, **'Industry of company'**, and **'Focus functions of company'**, we will drop the first two because they contain more missing values.\n",
    "\n",
    "3. **Remove 'Specialization of highest education'** because the information is too scattered and there are many different delimiters, making further examination of the column almost imposible.\n",
    "\n",
    "4. **For the remaining columns,** we will proceed with analysis as outlined in the cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ef38d-3d3a-4b14-a4d1-254737e3aea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "489318bc-86bd-4e8e-bf8c-7f8ee5ccbad8",
   "metadata": {},
   "source": [
    "# This code below works with a DataFrame `df2` and performs several tasks related to an 'Investors' column\n",
    "\n",
    "1. **Fill Missing Values:** Replaces missing values in the 'Investors' column with the most frequent value (mode).\n",
    "\n",
    "2. **Get Unique Investors:** Finds all unique investors in the 'Investors' column by splitting values on the '|' character.\n",
    "\n",
    "3. **Count Investor Occurrences:** Counts how often each unique investor appears in the 'Investors' column and prints the top 5 investors by count.\n",
    "\n",
    "4. **Create 'Yes' or 'No' Columns:** For a list of target investors, the code creates new columns in `df2` with values 'Yes' if the investor is in the 'Investors' column for a given row, and 'No' otherwise.\n",
    "\n",
    "5. **Copy Columns to Original DataFrame:** Copies the 'Yes'/'No' columns from `df2` to the original DataFrame `df`.\n",
    "\n",
    "6. **Show Updated DataFrame:** Displays the first few rows of the updated DataFrame.\n",
    "\n",
    "**we are working with a copy of the dataframe because the dataframe itself has changed after capitalization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9fa6a-10ee-4cb0-a9a8-6cfdf554c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the 'Investors' column with the mode (most frequent value)\n",
    "df2['Investors'].fillna(df2['Investors'].mode()[0], inplace=True)\n",
    "\n",
    "# Get a list of all unique investors from the 'Investors' column\n",
    "investors_list = df2['Investors'].tolist()\n",
    "unique_investors = set()\n",
    "\n",
    "# Populate the set of unique investors by splitting the 'Investors' column values by '|'\n",
    "for investor_entry in investors_list:\n",
    "    for investor in investor_entry.split('|'):\n",
    "        unique_investors.add(investor.strip())\n",
    "\n",
    "# Print the count of unique investors\n",
    "print(f\"Number of unique investors: {len(unique_investors)}\")\n",
    "\n",
    "# Create a dictionary to store the count of each unique investor in the 'Investors' column\n",
    "investor_count = {}\n",
    "\n",
    "# Count the occurrences of each unique investor in the 'Investors' column\n",
    "for investor in unique_investors:\n",
    "    count = sum(investor in entry for entry in investors_list)\n",
    "    investor_count[investor] = count\n",
    "\n",
    "# Sort the dictionary by count in descending order and print the top 40 investors\n",
    "sorted_investor_count = sorted(investor_count.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 5 investors by count:\")\n",
    "print(sorted_investor_count[:5])\n",
    "\n",
    "# List of investor names for which we want to create 'Yes' or 'No' columns\n",
    "target_investors = ['TechStars', 'SV Angel', '500 Startups', 'Sequoia Capital', 'Y Combinator']\n",
    "\n",
    "# Add columns with default value 'No\n",
    "for investor in target_investors:\n",
    "    df2[investor] = 'No'\n",
    "\n",
    "# Update columns to 'Yes' based on the presence of target investors in the 'Investors' column\n",
    "for index, row in df2.iterrows():\n",
    "    for investor in target_investors:\n",
    "        if investor in row['Investors']:\n",
    "            df2.at[index, investor] = 'Yes'\n",
    "\n",
    "# Copy the new columns to df\n",
    "df[target_investors] = df2[target_investors]\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589c9ce-2686-421f-8d88-b7e2fff25efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515d80a-6aaa-491d-b164-3b673cc0db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the 'Focus functions of company' column with the most frequent value (mode)\n",
    "df['Focus functions of company'].fillna(df['Focus functions of company'].mode()[0], inplace=True)\n",
    "\n",
    "# Calculate value counts for the 'Focus functions of company' column\n",
    "focus_function_counts = df['Focus functions of company'].value_counts()\n",
    "\n",
    "# Create a dictionary to replace values that occur once or less with 'other'\n",
    "replace_dict = {function: 'other' for function, count in focus_function_counts.items() if count <= 1}\n",
    "\n",
    "# Replace values in the 'Focus functions of company' column according to the dictionary\n",
    "df['Focus functions of company'] = df['Focus functions of company'].replace(replace_dict)\n",
    "\n",
    "# Print the updated value counts for the 'Focus functions of company' column\n",
    "print(df['Focus functions of company'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9ebb7-8f55-4904-9989-f9fa98ef329c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a8395-0f7c-4bce-a136-4f6aa040577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'last founding date' column into year and month\n",
    "temp_list = df['Last Funding Date'].str.split('/', expand=True)\n",
    "# Creating new columns with last funding month and last funding year\n",
    "df['Last_Funding_month'] = temp_list[0]\n",
    "df['Last_Funding_year'] = temp_list[2]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9d751-3aac-452c-a25d-5d62c4517ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3fdac70-ec60-4478-9cac-49fa857f497e",
   "metadata": {},
   "source": [
    "# Now we will investigate Multicollinearity issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebb85e-3f5b-409c-899b-cbe1696c6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.select_dtypes(include=['int', 'float']).corr()\n",
    "\n",
    "# Extract the upper triangle of the correlation matrix (excluding the diagonal)\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Sort the correlations in descending order\n",
    "sorted_corr = upper_triangle.unstack().sort_values(ascending=False)\n",
    "\n",
    "# Select the top 10 correlations\n",
    "top_15_corr = sorted_corr.head(15)\n",
    "\n",
    "print(\"Top 15 largest correlation coefficients:\")\n",
    "print(top_15_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b76a773-f564-44e6-a5be-c93d652529ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_in_top_15_corr = set(top_15_corr.index.get_level_values(0)).union(set(top_15_corr.index.get_level_values(1)))\n",
    "columns_in_top_15_corr = list(columns_in_top_15_corr)\n",
    "print(columns_in_top_15_corr)\n",
    "count_nan_values(df,columns_in_top_15_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411be055-bffe-44f4-8fa7-a60ee0e42815",
   "metadata": {},
   "source": [
    "# Here we decided on the following actions:\n",
    "\n",
    "1. **Remove 'Employees per year of company existence' and 'Last round of funding received (in milionUSD)' due to a large number of missing values.**\n",
    "\n",
    "2. **Remove 'Time to 1st investment' because the difference in missing values between 'Avg time to investment' and 'Time to 1st investment' is small, and 'Avg time to investment' provides more comprehensive information.**\n",
    "\n",
    "3. **Remove 'Experience in Fortune 100 organizations' and 'Experience in Fortune 1000 organizations' and leave 'Experience in Fortune 500 organizations'.**\n",
    "\n",
    "4. **Keep all remaining columns because any correlation observed is likely random.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0277464-ad18-4ea3-a3f9-b95549c15229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8542277-fc68-4eb8-9527-4f8310c01662",
   "metadata": {},
   "source": [
    "**Here below we will visualize all numerical variables using an interactive dashboard with two strategies such as boxplot and histogram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e4d307-3aee-4c01-affd-ec32caa7c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_N = df.select_dtypes(include=['int', 'float'])\n",
    "num_pages = len(df_N.columns)//4+1\n",
    "strategy_dropdown = widgets.Dropdown(options=['Histogram', 'Box Plot'], description='Strategy:')\n",
    "page_dropdown = widgets.Dropdown(options=list(range(1, num_pages + 1)), description='Page:')\n",
    "\n",
    "# Function to update plots based on selected strategy and page\n",
    "def update_plots(strategy, page):\n",
    "    start_index = (page - 1) * 4\n",
    "    end_index = min(page * 4, len(df_N.columns))\n",
    "    columns_to_display = df_N.columns[start_index:end_index]\n",
    "\n",
    "    # Split columns for top and bottom plots\n",
    "    mid = len(columns_to_display) // 2\n",
    "    top_columns = columns_to_display[:mid]\n",
    "    bottom_columns = columns_to_display[mid:]\n",
    "\n",
    "    # Create subplots for top and bottom plots\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    # Update top plots\n",
    "    if len(top_columns) == 1:\n",
    "        plot_axes = axes[0].flatten()\n",
    "        if strategy == 'Histogram':\n",
    "            sns.histplot(df_N[top_columns[0]], kde=True, ax=plot_axes[0])\n",
    "            plot_axes[0].set_title(f'Histogram of {top_columns[0]}')\n",
    "            plot_axes[0].set_xlabel(top_columns[0])\n",
    "            plot_axes[0].set_ylabel('Frequency')\n",
    "        elif strategy == 'Box Plot':\n",
    "            sns.boxplot(x=top_columns[0], data=df_N, ax=plot_axes[0])\n",
    "            plot_axes[0].set_title(f'Box Plot of {top_columns[0]}')\n",
    "            plot_axes[0].set_xlabel(top_columns[0])\n",
    "            plot_axes[0].set_ylabel('Values')\n",
    "    else:\n",
    "        for i, col in enumerate(top_columns):\n",
    "            plot_axes = axes[0, i]\n",
    "            if strategy == 'Histogram':\n",
    "                sns.histplot(df_N[col], kde=True, ax=plot_axes)\n",
    "                plot_axes.set_title(f'Histogram of {col}')\n",
    "                plot_axes.set_xlabel(col)\n",
    "                plot_axes.set_ylabel('Frequency')\n",
    "            elif strategy == 'Box Plot':\n",
    "                sns.boxplot(x=col, data=df_N, ax=plot_axes)\n",
    "                plot_axes.set_title(f'Box Plot of {col}')\n",
    "                plot_axes.set_xlabel(col)\n",
    "                plot_axes.set_ylabel('Values')\n",
    "\n",
    "    # Update bottom plots\n",
    "    if len(bottom_columns) == 1:\n",
    "        plot_axes = axes[1].flatten()\n",
    "        if strategy == 'Histogram':\n",
    "            sns.histplot(df_N[bottom_columns[0]], kde=True, ax=plot_axes[0])\n",
    "            plot_axes[0].set_title(f'Histogram of {bottom_columns[0]}')\n",
    "            plot_axes[0].set_xlabel(bottom_columns[0])\n",
    "            plot_axes[0].set_ylabel('Frequency')\n",
    "        elif strategy == 'Box Plot':\n",
    "            sns.boxplot(x=bottom_columns[0], data=df_N, ax=plot_axes[0])\n",
    "            plot_axes[0].set_title(f'Box Plot of {bottom_columns[0]}')\n",
    "            plot_axes[0].set_xlabel(bottom_columns[0])\n",
    "            plot_axes[0].set_ylabel('Values')\n",
    "    else:\n",
    "        for i, col in enumerate(bottom_columns):\n",
    "            plot_axes = axes[1, i]\n",
    "            if strategy == 'Histogram':\n",
    "                sns.histplot(df_N[col], kde=True, ax=plot_axes)\n",
    "                plot_axes.set_title(f'Histogram of {col}')\n",
    "                plot_axes.set_xlabel(col)\n",
    "                plot_axes.set_ylabel('Frequency')\n",
    "            elif strategy == 'Box Plot':\n",
    "                sns.boxplot(x=col, data=df_N, ax=plot_axes)\n",
    "                plot_axes.set_title(f'Box Plot of {col}')\n",
    "                plot_axes.set_xlabel(col)\n",
    "                plot_axes.set_ylabel('Values')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Interactivity\n",
    "widgets.interactive(update_plots, strategy=strategy_dropdown, page=page_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5083c32-785e-472e-84a2-b9a5a02044a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Renowned in professional circle'] = np.where(df['Renowned in professional circle'] > 450, 'great than 450', 'less than 450')\n",
    "df['Percent_skill_Finance'] = np.where(df['Percent_skill_Finance'] < 15, 'less than 15', 'great than 15')\n",
    "df['Percent_skill_Investment'] = np.where(df['Percent_skill_Investment'] < 15, 'less than 15', 'great than 15')\n",
    "df['Percent_skill_Law'] = np.where(df['Percent_skill_Law'] < 7, 'less than 7', 'great than 7')\n",
    "df['Percent_skill_Consulting'] = np.where(df['Percent_skill_Consulting'] < 6, 'less than 6', 'great than 6')\n",
    "df['Number of Investors in Angel and or VC'] = np.where(df['Number of Investors in Angel and or VC'] <= 1.5, 'less than 1.5', 'great than 1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b42634-5093-4ed4-94c6-8f62b31c3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_N=df.select_dtypes(include=['int', 'float'])\n",
    "for feature in df_N.columns:\n",
    "        # Apply log transformation to the feature\n",
    "        df[feature] = np.log1p(df[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa65005-c6c6-4bfa-8831-92af963e2b71",
   "metadata": {},
   "source": [
    "**we see that there are a lot of columns with significantly many outlayers, so we will apply a logarithmic transformation, but even after this there are columns with outlayers, so before the transformation we will change the values ​​of these columns to \"less then threshold\" and \"great then threshold\" to solve this problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5316818-9464-44c0-8bb5-2623d1ab00d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interactive(update_plots, strategy=strategy_dropdown, page=page_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ad3949-6bc7-4aaf-b6c2-12612f0d3739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Run the droping function to remove the unnecessary columns identified above.\"\n",
    "droping(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57b7f15-f60c-4b5b-9d03-bb44782e6094",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_nan_with_median(df):\n",
    "    \"\"\"\n",
    "    Replace NaN values in numeric columns with the median value of each column.\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid modifying the original DataFrame\n",
    "    df_filled = df.copy()\n",
    "    \n",
    "    # Get numeric columns\n",
    "    numeric_columns = df_filled.select_dtypes(include=['int', 'float']).columns\n",
    "    \n",
    "    # Replace NaN values with column-wise median\n",
    "    for col in numeric_columns:\n",
    "        median_value = df_filled[col].median()\n",
    "        df_filled[col].fillna(median_value, inplace=True)\n",
    "    \n",
    "    return df_filled\n",
    "df=replace_nan_with_median(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9dc4af-7097-491d-bdb9-1cb7b70b6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_with_mode(df):\n",
    "    \"\"\"\n",
    "    Replace NaN values in object columns with the mode (most frequent value) of each column.\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid modifying the original DataFrame\n",
    "    df_filled = df.copy()\n",
    "    \n",
    "    # Get object columns\n",
    "    object_columns = df_filled.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Replace NaN values with column-wise mode\n",
    "    for col in object_columns:\n",
    "        mode_value = df_filled[col].mode()[0]  # Use [0] to get the first mode if multiple modes exist\n",
    "        df_filled[col].fillna(mode_value, inplace=True)\n",
    "    \n",
    "    return df_filled\n",
    "df=replace_nan_with_mode(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7d2d7-380d-4a84-bd3f-a12a69027c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8fa39d-ef8c-4d0f-9a77-6ce0c3af4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44137873-b886-4dbe-9738-d7a32e81f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling infinite values, impute missing values, and encode categorical data.\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_columns = df.select_dtypes(include=['int', 'float']).columns\n",
    "\n",
    "# Impute missing values in numeric columns using the median strategy\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Encode categorical columns using ordinal encoding\n",
    "encoder = OrdinalEncoder()\n",
    "df_encoded = encoder.fit_transform(df)\n",
    "df_encoded = pd.DataFrame(df_encoded, columns=df.columns)\n",
    "\n",
    "# Display the first 10 rows of the encoded DataFrame\n",
    "df_encoded.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6902cb7c-8511-489c-bb24-7c21ac7f2d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13062875-761d-454a-b42d-6c5715da89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "\n",
    "X = df_encoded.drop('Dependent-Company Status', axis = 1) \n",
    "\n",
    "y = df_encoded['Dependent-Company Status'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e2412-0d3e-493d-bffa-0bd49a974f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14b03f-fac4-464e-ab89-7cba1d8e90ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85616ce-2815-4fb3-9fb9-6b245d713bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets print the shapes\n",
    "print(\"Shape of the X Train :\", X_train.shape)\n",
    "print(\"Shape of the y Train :\", y_train.shape)\n",
    "print(\"Shape of the X test :\", X_test.shape)\n",
    "print(\"Shape of the y test :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6b241c-b46e-47c7-928e-6d92d982bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score,roc_curve, auc, precision_recall_curve, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db58c04-22ca-4f13-95d8-284ba037a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d443d2-28f8-4c66-93bd-758cfe335442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "#train\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb.fit(X_train,y_train)\n",
    "\n",
    "#predict\n",
    "y_predicted_xgb = xgb.predict(X_test)\n",
    "\n",
    "print(\"Training Accuracy :\", xgb.score(X_train, y_train))\n",
    "print(\"Testing Accuracy :\", xgb.score(X_test, y_test))\n",
    "\n",
    "#eval\n",
    "cm = confusion_matrix(y_test, y_predicted_xgb)\n",
    "plt.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.heatmap(cm, annot = True, cmap = 'YlGnBu', fmt = '.8g')\n",
    "plt.show()\n",
    "\n",
    "cr = classification_report(y_test, y_predicted_xgb)\n",
    "print(cr)\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_predicted_xgb)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"ROC Curves              =\",roc_auc)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_predicted_xgb)\n",
    "f1 = f1_score(y_test, y_predicted_xgb)\n",
    "Precision_Recall_xgb = auc(recall, precision)\n",
    "print(\"Precision-Recall Curves =\",Precision_Recall_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afbabb8-dae3-4f0b-95b2-8be80b603b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#train\n",
    "gbc = GradientBoostingClassifier(learning_rate=0.02,\n",
    "                    max_depth=4,\n",
    "                    random_state=100, n_estimators=1000)\n",
    "\n",
    "\n",
    "gbc.fit(X_train,y_train)\n",
    "\n",
    "#predict\n",
    "y_predicted_gb = gbc.predict(X_test)\n",
    "\n",
    "print(\"Training Accuracy :\", gbc.score(X_train, y_train))\n",
    "print(\"Testing Accuracy :\", gbc.score(X_test, y_test))\n",
    "\n",
    "#eval\n",
    "cm = confusion_matrix(y_test, y_predicted_gb)\n",
    "plt.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.heatmap(cm, annot = True, cmap = 'YlGnBu', fmt = '.8g')\n",
    "plt.show()\n",
    "\n",
    "cr = classification_report(y_test, y_predicted_gb)\n",
    "print(cr)\n",
    "\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_predicted_gb)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"ROC Curves              =\",roc_auc)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_predicted_gb)\n",
    "f1 = f1_score(y_test, y_predicted_gb)\n",
    "Precision_Recall_gbs = auc(recall, precision)\n",
    "print(\"Precision-Recall Curves =\",Precision_Recall_gbs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b373105-24cc-4bfe-a24b-cd326b3cbf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#train\n",
    "ada = AdaBoostClassifier()\n",
    "\n",
    "\n",
    "ada.fit(X_train,y_train)\n",
    "\n",
    "#predict\n",
    "y_predicted_ab = ada.predict(X_test)\n",
    "\n",
    "print(\"Training Accuracy :\", ada.score(X_train, y_train))\n",
    "print(\"Testing Accuracy :\", ada.score(X_test, y_test))\n",
    "\n",
    "#eval\n",
    "cm = confusion_matrix(y_test, y_predicted_ab)\n",
    "plt.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.heatmap(cm, annot = True, cmap = 'YlGnBu', fmt = '.8g')\n",
    "plt.show()\n",
    "\n",
    "cr = classification_report(y_test, y_predicted_ab)\n",
    "print(cr)\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_predicted_ab)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"roc_auc\",roc_auc)\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_predicted_ab)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"ROC Curves              =\",roc_auc)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_predicted_ab)\n",
    "f1 = f1_score(y_test, y_predicted_ab)\n",
    "Precision_Recall_abs = auc(recall, precision)\n",
    "print(\"Precision-Recall Curves =\",Precision_Recall_abs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765b0a8-df99-48f7-bc3a-7065efefd6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Training Accuracy :\", rf.score(X_train, y_train))\n",
    "print(\"Testing Accuracy :\", rf.score(X_test, y_test))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.heatmap(cm, annot = True, cmap = 'YlGnBu', fmt = '.8g')\n",
    "plt.show()\n",
    "\n",
    "cr = classification_report(y_test, y_pred_rf)\n",
    "print(cr)\n",
    "\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_pred_rf)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"ROC Curves              =\",roc_auc)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_rf)\n",
    "f1 = f1_score(y_test, y_pred_rf)\n",
    "Precision_Recall_rfs = auc(recall, precision)\n",
    "print(\"Precision-Recall Curves =\",Precision_Recall_rfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03bab8-529f-4ad1-b728-e87648bce2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "scores = {        \n",
    "                 'GradientBoosting Classifier':  { \n",
    "                             'precision_score': precision_score(y_test, y_predicted_gb),\n",
    "                             'recall_score': recall_score(y_test, y_predicted_gb)\n",
    "                         },\n",
    "                 'Adaboost Classifier':  { \n",
    "                             'precision_score': precision_score(y_test, y_predicted_ab),\n",
    "                             'recall_score': recall_score(y_test, y_predicted_ab)\n",
    "                         },\n",
    "                 'XGBoost':  { \n",
    "                             'precision_score': precision_score(y_test, y_predicted_xgb),\n",
    "                             'recall_score': recall_score(y_test, y_predicted_xgb)\n",
    "                         },\n",
    "                 'Random Forest':  { \n",
    "                             'precision_score': precision_score(y_test, y_pred_rf),\n",
    "                            'recall_score': recall_score(y_test, y_pred_rf)\n",
    "                         }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3705cc-6d63-435a-b799-160fc689cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "Precision_Recall = {\n",
    "    \n",
    "                 'GradientBoosting Classifier':  { \n",
    "                             'Precision_Recall': Precision_Recall_gbs\n",
    "                         },\n",
    "                 'Adaboost Classifier':  { \n",
    "                             'Precision_Recall': Precision_Recall_abs\n",
    "                         },\n",
    "                 'XGBoost':  { \n",
    "                             'Precision_Recall': Precision_Recall_xgb\n",
    "                         },\n",
    "                 'Random Forest':  { \n",
    "                             'Precision_Recall': Precision_Recall_rfs\n",
    "                         }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9625ce-254c-42db-a984-9f964485f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(scores)\n",
    "\n",
    "\n",
    "scores.plot(kind=\"barh\",figsize=(12, 12)).legend(loc='upper center', ncol=3, title=\"Machine Learning Model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ce966-5107-49a8-ac64-a12c5bdecaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision_Recall = pd.DataFrame(Precision_Recall)\n",
    "\n",
    "\n",
    "Precision_Recall.plot(kind=\"barh\",figsize=(15, 8)).legend(loc='upper center', ncol=3, title=\"Machine Learning Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e8c5bd-9f07-4e15-9eb7-6c6ba3d2d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef82fa9-4a40-4c9d-b5cb-c9136f5f9cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
